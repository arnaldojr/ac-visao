{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESSAMENTO DE IMAGENS \n",
    "\n",
    "**Objetivos da aula:**\n",
    "\n",
    "*   apresentar e aplicar a lib `Dlib` para detecção de face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Não se vive vive apenas de HAAR CASCADE**\n",
    "\n",
    "Muito obrigado Viola e Jones! Se não fosse por vocês, com o `Haar Cascade` em 2001, não teriamos hoje em dia bibliotecas tão incriveis para detecção de face.\n",
    "\n",
    "\n",
    "Atualmente, existem diversas bibliotecas disponíveis para detecção não apenas de face mas tambem de mãos, corpo, objetos...\n",
    "\n",
    "\n",
    "Vou destacar algumas redes:\n",
    "\n",
    "   ***[Dlib C++](https://github.com/davisking/dlib)***\n",
    "   \n",
    "   ***[MTCNN](https://github.com/ipazc/mtcnn)***\n",
    "   \n",
    "   ***[Media Pipe](https://github.com/google/mediapipe)*** \n",
    "   \n",
    "\n",
    "\n",
    "Cada uma dessas bibliotecas, usam técnicas de `machine learning` na etapa de treinamento para realizar a predição de **face**.\n",
    "\n",
    "Hoje vamos da destaque especial para a `Dlid`. Masssssss....como sugestão, leia a documetação e aprenda a usar a MTCNN, Media Pipe e a clássica HaarCascade. Como resultado, descubra qual é mais rápida, mais leve ou mais acurada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dlib\n",
    "\n",
    "A Dlib, além de realizar a predição para detectar uma face, ela consegue ressaltar alguns pontos-chaves da face. Esses pontos de destaque são chamados de **landmarks**, a Dlib detecta 68 landmarks como cantos dos olhos, sobrancelhas, boca e ponta do nariz.\n",
    "\n",
    "Cada landmark devolve uma coordenada da posição (x,y) da imagem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"landmark.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalação Dlib\n",
    "\n",
    "A instalação é feita com o comando:\n",
    "   \n",
    "```bash\n",
    "pip install dlib\n",
    "    \n",
    "```\n",
    "\n",
    "### Verificação da instalação\n",
    "\n",
    "\n",
    "<img src=\"installdlib.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deteção de faces com o dlib\n",
    "\n",
    "Vamos fazer o nosso \"Hello World\" usando a Dlib, neste caso, vamos implementar um detector de face simples.\n",
    "\n",
    "Para quem já usou o HaarCascade, note que o método é bem similar, apenas a lista de retorno das faces detectadas é um pouco diferente. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Importa a Dlib\n",
    "import dlib\n",
    " \n",
    "# carrega uma imagem \n",
    "img = cv2.imread('lena.png')\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    \n",
    "  \n",
    "    \n",
    "# Inicializa o detector dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Faz deteção das faces\n",
    "faces = detector(img_gray)\n",
    "\n",
    "print(faces)\n",
    "\n",
    "# Faz uma cópia da imagem original\n",
    "img1  = img.copy()\n",
    "\n",
    "for face in faces:\n",
    "        x,y = face.left(), face.top()  # topo esquerda\n",
    "        x1,y1 = face.right(), face.bottom() #baixo direita\n",
    "        \n",
    "        cv2.rectangle(img1, (x, y), (x1, y1), (0, 255, 0), 4)\n",
    "\n",
    "\n",
    "# Exibe imagem \n",
    "plt.figure(figsize = (20,20))\n",
    "plt.subplot(121);plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB));\n",
    "plt.subplot(122);plt.imshow(cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)); plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deteção de face com a webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Cria a captura de video da webcam\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Inicializa o detector dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "\n",
    "while True:\n",
    "    # pega frame da webcam:\n",
    "    ret, frame = capture.read()\n",
    "    \n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "    \n",
    "    # Faz deteção das faces\n",
    "    faces = detector(frame_gray)\n",
    "\n",
    "    for face in faces:\n",
    "            x,y = face.left(), face.top()  # topo esquerda\n",
    "            x1,y1 = face.right(), face.bottom() #baixo direita\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x1, y1), (0, 255, 0), 4)\n",
    "\n",
    "            \n",
    "    # Exibe o resultado:\n",
    "    cv2.imshow(\"face\", frame)\n",
    "\n",
    "    # Lê o teclado\n",
    "    key = 0xFF & cv2.waitKey(1)    \n",
    "    if key == ord('q'): # aperte 'q' para sair\n",
    "        break\n",
    "    \n",
    "    \n",
    "# Fecha programa:\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecção dos landmarks da face \n",
    "\n",
    "Para predição dos landmarks uma rede neural já treinada é utilizada, serão preditos 68 pontos da face. \n",
    "\n",
    "Para carregar os pesos da rede é necessário fazer o download em:\n",
    "\n",
    "[hape_predictor_68_face_landmarks.dat](http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2)\n",
    "\n",
    "#### Dlib positions\n",
    "- (\"mouth\", (48, 68)),\n",
    "- (\"right_eyebrow\", (17, 22)),\n",
    "- (\"left_eyebrow\", (22, 27)),\n",
    "- (\"right_eye\", (36, 42)),\n",
    "- (\"left_eye\", (42, 48)),\n",
    "- (\"nose\", (27, 35)),\n",
    "- (\"jaw\", (0, 17))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import dlib\n",
    " \n",
    "# carrega uma imagem para detectar o rosto\n",
    "img = cv2.imread('lena.png')\n",
    "\n",
    "# Faz uma cópia da imagem original\n",
    "img1  = img.copy()\n",
    "img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)    \n",
    "\n",
    "\n",
    "\n",
    "# Inicializa o detector dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Inicializa o identificador de Landmark. Você deve ter esse arquivo na pasta do projeto.\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\") \n",
    "\n",
    "\n",
    "# Faz deteção das faces\n",
    "faces = detector(img1_gray)\n",
    "\n",
    "print(faces)\n",
    "\n",
    "\n",
    "for face in faces:\n",
    "    # Faz a predição dos landmarks\n",
    "    shape = predictor(img1_gray, face) \n",
    "    for i in range(0,68): #São 68 landmark em cada face\n",
    "        # Desenha um circulo e exibe o indice do landmark\n",
    "        # shape.part(i).x devolve o valor x da coordenada\n",
    "        cv2.circle(img1, (shape.part(i).x, shape.part(i).y), 1, (0,255,0), thickness=-1)\n",
    "        \n",
    "        # Escreve o indice de cada landmark na imagem\n",
    "        cv2.putText(img1, str(i), (shape.part(i).x,shape.part(i).y), fontFace=cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, fontScale=0.3, color=(0, 0, 255))\n",
    "\n",
    "        \n",
    "        \n",
    "# Exibe imagem \n",
    "plt.figure(figsize = (20,20))\n",
    "plt.subplot(121);plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB));\n",
    "plt.subplot(122);plt.imshow(cv2.cvtColor(img1,cv2.COLOR_BGR2RGB)); plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafio 1\n",
    "\n",
    "Faça um código capaz de detectar em tempo real as landmarks face pela webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implemente sua solução...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking\n",
    "\n",
    "O tracking de objetos ou de faces possui diversas aplicações.O rastreamento de objetos tenta estimar a trajetória do alvo ao longo da sequência de vídeo onde apenas a localização inicial de um alvo é conhecida. Basicamente o custo computacional para realização de rastreamento de um objeto é muito menor, o que é crítico em aplicações em tempo real, comparado ao custo computacional para a detecção de um objeto, onde a cada frame é realizado a detecção do zero.  \n",
    " \n",
    "Uma técnica muito poderosa para a realização de rastreamento é DCF (discriminative correlation filter).A biblioteca dlib implementa um rastreador baseado em DCF, que é fácil de usar para rastreamento de faces ou objetos.\n",
    "\n",
    "Artigo bacana para se aprofundar na teoria:\n",
    "https://arxiv.org/pdf/1611.08461.pdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rastreamento de face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "\n",
    "def draw_text_info():\n",
    "    \"\"\"Função para escrever na tela as instruções de uso\"\"\"\n",
    "\n",
    "    # define posições na tela\n",
    "    menu_pos_1 = (10, 20)\n",
    "    menu_pos_2 = (10, 40)\n",
    "\n",
    "    cv2.putText(frame, \"Use '1' para re-inicializar tracking\", menu_pos_1, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))\n",
    "    if tracking_face:\n",
    "        cv2.putText(frame, \"Rastreando...\", menu_pos_2, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0))\n",
    "    else:\n",
    "        cv2.putText(frame, \"Procurando...\", menu_pos_2, cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                    (0, 0, 255))\n",
    "\n",
    "\n",
    "# Cria a captura de video da webcam\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Inicializa o detector dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Inicializa o correlation tracker.\n",
    "tracker = dlib.correlation_tracker()\n",
    "\n",
    "# Estado do tracking:\n",
    "tracking_face = False\n",
    "\n",
    "while True:\n",
    "    # pega frame da webcam:\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    # Escreve na tela instruções de uso:\n",
    "    draw_text_info()\n",
    "\n",
    "    if tracking_face is False:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray, 0)\n",
    "        # se achou face \n",
    "        if len(rects) > 0:\n",
    "            # Começa o tracking:\n",
    "            tracker.start_track(frame, rects[0])\n",
    "            tracking_face = True\n",
    "\n",
    "    if tracking_face is True:\n",
    "        # Atualiza o rastreamento.\n",
    "        tracker.update(frame)\n",
    "        # Pega as posições\n",
    "        pos = tracker.get_position()\n",
    "        cv2.rectangle(frame, (int(pos.left()), int(pos.top())), (int(pos.right()), int(pos.bottom())), (0, 255, 0), 3)\n",
    "\n",
    "    # Lê o teclado\n",
    "    key = 0xFF & cv2.waitKey(1)\n",
    "\n",
    "    # tecla 1 - reinicia tudo\n",
    "    if key == ord(\"1\"):\n",
    "        tracking_face = False\n",
    "\n",
    "    \n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Show the resulting image:\n",
    "    cv2.imshow(\"tracking\", frame)\n",
    "\n",
    "# Release everything:\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rastreamento de objetos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "\n",
    "def draw_text_info():\n",
    "    \"\"\"Função para escrever na tela as instruções de uso\"\"\"\n",
    "\n",
    "    menu_pos = (10, 20)\n",
    "    menu_pos_2 = (10, 40)\n",
    "    menu_pos_3 = (10, 60)\n",
    "    info_1 = \"Selecione com o mouse o objeto para rastreamento\"\n",
    "    info_2 = \"Use '1' para start, '2' para reset\"\n",
    "\n",
    "    cv2.putText(frame, info_1, menu_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))\n",
    "    cv2.putText(frame, info_2, menu_pos_2, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))\n",
    "    if tracking_state:\n",
    "        cv2.putText(frame, \"Rastreando...\", menu_pos_3, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0))\n",
    "    else:\n",
    "        cv2.putText(frame, \"Não rastreando\", menu_pos_3, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))\n",
    "\n",
    "\n",
    "\n",
    "points = []\n",
    "\n",
    "\n",
    "# Função de callback do mouse\n",
    "def mouse_click(event, x, y, flags, param):\n",
    "    global points\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        points = [(x, y)]\n",
    "\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        points.append((x, y))\n",
    "\n",
    "\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "window_name = \"tracking\"\n",
    "cv2.namedWindow(window_name)\n",
    "cv2.setMouseCallback(window_name, mouse_click)\n",
    "\n",
    "# Inicializa metodo de correlação de rastreamento\n",
    "tracker = dlib.correlation_tracker()\n",
    "\n",
    "# Variavel de estado\n",
    "tracking_state = False\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    draw_text_info()\n",
    "\n",
    "    # Se objeto esta selecionado\n",
    "    if len(points) == 2:\n",
    "        cv2.rectangle(frame, points[0], points[1], (0, 0, 255), 3)\n",
    "        dlib_rectangle = dlib.rectangle(points[0][0], points[0][1], points[1][0], points[1][1])\n",
    "\n",
    "    # Se é pra rastrear\n",
    "    if tracking_state == True:\n",
    "        tracker.update(frame)\n",
    "        pos = tracker.get_position()\n",
    "        cv2.rectangle(frame, (int(pos.left()), int(pos.top())), (int(pos.right()), int(pos.bottom())), (0, 255, 0), 3)\n",
    "\n",
    "    # lê teclado\n",
    "    key = 0xFF & cv2.waitKey(1)\n",
    "\n",
    "    # '1' start\n",
    "    if key == ord(\"1\"):\n",
    "        if len(points) == 2:\n",
    "            # Start tracking:\n",
    "            tracker.start_track(frame, dlib_rectangle)\n",
    "            tracking_state = True\n",
    "            points = []\n",
    "\n",
    "    # '2' reset\n",
    "    if key == ord(\"2\"):\n",
    "        points = []\n",
    "        tracking_state = False\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    cv2.imshow(window_name, frame)\n",
    "\n",
    "# Release everything:\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector de fadiga\n",
    "\n",
    "O detector de fadiga pode ser elaborado a partir a abertura dos olhos, o código a baixo foi inspirado no link:\n",
    "[Artigo de refência](https://bit.ly/2CYC7Gf)\n",
    "\n",
    "\n",
    "\n",
    "Além deste artigo, vamos utilizar algumas outras funções da OpenCV que ainda não conhecemos. \n",
    "\n",
    "cv2.convexHull = Cria um contorno com base nos pontos. https://learnopencv.com/convex-hull-using-opencv-in-python-and-c/\n",
    "\n",
    "\n",
    "#### Dlib positions\n",
    "- (\"mouth\", (48, 68)),\n",
    "- (\"right_eyebrow\", (17, 22)),\n",
    "- (\"left_eyebrow\", (22, 27)),\n",
    "- (\"right_eye\", (36, 42)),\n",
    "- (\"left_eye\", (42, 48)),\n",
    "- (\"nose\", (27, 35)),\n",
    "- (\"jaw\", (0, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import dist\n",
    "import time\n",
    "import numpy as np\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "# definir constantes\n",
    "EYE_AR_THRESH = 0.3\n",
    "EYE_AR_CONSEC_FRAMES = 40\n",
    "COUNTER = 0\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    # calcula a distancia euclidiana vertical os olhos\n",
    "    # vertical eye landmarks (x, y)-coordinates\n",
    "    A = dist(eye[1], eye[5])\n",
    "    B = dist(eye[2], eye[4])\n",
    "\n",
    "    # # calcula a distancia euclidiana horizontal os olhos\n",
    "    # eye landmark (x, y)-coordinates\n",
    "    C = dist(eye[0], eye[3])\n",
    "\n",
    "    # calcula uma taxa de abertura dos olhos\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "\n",
    "    # return the eye aspect ratio\n",
    "    return ear\n",
    "\n",
    "\n",
    "# inicializa o detector e preditor do dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# pega os índices do previsor, para olhos esquerdo e direito\n",
    "(lStart, lEnd) = (42, 48)\n",
    "(rStart, rEnd) = (36, 42)\n",
    "\n",
    "# inicializar vídeo\n",
    "vs = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# loop sobre os frames do vídeo\n",
    "while True:\n",
    "    ret, frame = vs.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detectar faces (grayscale)\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    # loop nas detecções de faces\n",
    "    for rect in rects:\n",
    "        \n",
    "        shape = predictor(gray, rect)\n",
    "        #devolve shape em uma lista coords\n",
    "        coords = np.zeros((shape.num_parts, 2), dtype=int)\n",
    "        for i in range(0,68): #São 68 landmark em cada face\n",
    "            coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\n",
    "        # extrair coordenadas dos olhos e calcular a proporção de abertura\n",
    "        leftEye = coords[lStart:lEnd]\n",
    "        rightEye = coords[rStart:rEnd]\n",
    "        \n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "\n",
    "        # ratio média para os dois olhos\n",
    "        ear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "        # convex hull cria um contorno com base nos pontos \n",
    "        leftEyeHull = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "\n",
    "        # checar ratio x threshold\n",
    "        if ear < EYE_AR_THRESH:\n",
    "            COUNTER += 1\n",
    "\n",
    "            # dentro dos critérios\n",
    "            if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                cv2.putText(frame, \"[ALERTA] FADIGA!\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # caso acima do threshold, resetar o contador e desligar o alarme\n",
    "        else:\n",
    "            COUNTER = 0\n",
    "            # desenhar a proporção de abertura dos olhos\n",
    "        cv2.putText(frame, \"EAR: {:.2f}\".format(ear), (300, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    # Exibe resultado\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # tecla para sair do script \"q\"\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# clean\n",
    "cv2.destroyAllWindows()\n",
    "vs.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desafio 2 \n",
    "\n",
    "Inspirado na solução do detector de fadiga, implemente um código que faz a deteção de emoção. Ou seja, detecta se a pessoa esta sorrindo ou não.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemente sua solução aqui......\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Visão_Computacional_Aula_13.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
